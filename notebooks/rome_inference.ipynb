{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROME: Realistic one-shot mesh-based head avatars\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![extreme](../media/tease0.gif)\n",
    "\n",
    "[ROME](https://samsunglabs.github.io/rome/) Project Page Don't forget to check orignal [DECA](https://github.com/YadiraF/DECA/) project.\n",
    "\n",
    "[![Taras](https://img.shields.io/twitter/follow/t_khakhulin?style=social)](https://twitter.com/t_khakhulin)\n",
    "\n",
    "[![GitHub stars](https://img.shields.io/github/stars/samsunglabs/rome?style=social)](https://github.com/samsunglabs/rome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import importlib, warnings\n",
    "import argparse\n",
    "from glob import glob\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "from src.utils import args as args_utils\n",
    "from src.utils.processing import process_black_shape, tensor2image\n",
    "from src.utils.visuals import mask_errosion\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_modnet_path = 'MODNet/pretrained/modnet_photographic_portrait_matting.ckpt'\n",
    "default_model_path = 'data/rome.pth'\n",
    "\n",
    "parser = argparse.ArgumentParser(conflict_handler='resolve')\n",
    "parser.add_argument('--save_dir', default='.', type=str)\n",
    "parser.add_argument('--save_render', default='True', type=args_utils.str2bool, choices=[True, False])\n",
    "parser.add_argument('--model_checkpoint', default=default_model_path, type=str)\n",
    "parser.add_argument('--modnet_path', default=default_modnet_path, type=str)\n",
    "parser.add_argument('--random_seed', default=0, type=int)\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "parser.add_argument('--verbose', default='False', type=args_utils.str2bool, choices=[True, False])\n",
    "args, _ = parser.parse_known_args()\n",
    "# \n",
    "parser = importlib.import_module(f'src.rome').ROME.add_argparse_args(parser)\n",
    "\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "args.rome_data_dir = 'data'\n",
    "args.deca_path  = 'DECA'\n",
    "\n",
    "args.model_checkpoint = default_model_path\n",
    "args.modnet_path = default_modnet_path\n",
    "\n",
    "args.align_source = True\n",
    "args.align_scale = 1.25\n",
    "args.image_size = 256\n",
    "\n",
    "args.use_distill = True\n",
    "args.use_basis_deformer = False\n",
    "\n",
    "args.deform_face_tightness = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\human_mesh\\\\rome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n",
      "please check model path: DECA\\data\\deca_model.tar\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/linear_hair.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minfer\u001b[39;00m \u001b[39mimport\u001b[39;00m Infer\n\u001b[1;32m----> 2\u001b[0m infer \u001b[39m=\u001b[39m Infer(args)\n",
      "File \u001b[1;32mc:\\human_mesh\\rome\\infer.py:40\u001b[0m, in \u001b[0;36mInfer.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m     38\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInitialize model.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ROME(args)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     43\u001b[0m     \u001b[39m# transforms.Resize((256, 256)),\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m     45\u001b[0m ])\n",
      "File \u001b[1;32mc:\\human_mesh\\rome\\src\\rome.py:131\u001b[0m, in \u001b[0;36mROME.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[1;32m--> 131\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_networks(args)\n",
      "File \u001b[1;32mc:\\human_mesh\\rome\\src\\rome.py:208\u001b[0m, in \u001b[0;36mROME.init_networks\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasis_deformer \u001b[39m=\u001b[39m networks\u001b[39m.\u001b[39mEncoderResnet(\n\u001b[0;32m    196\u001b[0m         pretrained_encoder_basis_path\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mpretrained_encoder_basis_path,\n\u001b[0;32m    197\u001b[0m         norm_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgn+ws\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    198\u001b[0m         num_basis\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_basis)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvertex_deformer \u001b[39m=\u001b[39m networks\u001b[39m.\u001b[39mEncoderVertex(\n\u001b[0;32m    201\u001b[0m         path_to_deca_lib\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mpath_to_deca,\n\u001b[0;32m    202\u001b[0m         pretrained_vertex_basis_path\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mpretrained_vertex_basis_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         basis_init\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbasis_init,\n\u001b[0;32m    206\u001b[0m         num_vertex\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_vertex)\n\u001b[1;32m--> 208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparametric_avatar \u001b[39m=\u001b[39m ParametricAvatar(\n\u001b[0;32m    209\u001b[0m     args\u001b[39m.\u001b[39;49mmodel_image_size,\n\u001b[0;32m    210\u001b[0m     args\u001b[39m.\u001b[39;49mdeca_path,\n\u001b[0;32m    211\u001b[0m     args\u001b[39m.\u001b[39;49muse_scalp_deforms,\n\u001b[0;32m    212\u001b[0m     args\u001b[39m.\u001b[39;49muse_neck_deforms,\n\u001b[0;32m    213\u001b[0m     args\u001b[39m.\u001b[39;49msubdivide_mesh,\n\u001b[0;32m    214\u001b[0m     args\u001b[39m.\u001b[39;49muse_deca_details,\n\u001b[0;32m    215\u001b[0m     args\u001b[39m.\u001b[39;49muse_flametex,\n\u001b[0;32m    216\u001b[0m     args,\n\u001b[0;32m    217\u001b[0m     device\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mdevice,\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet \u001b[39m=\u001b[39m networks\u001b[39m.\u001b[39mUNet(\n\u001b[0;32m    221\u001b[0m     args\u001b[39m.\u001b[39munet_num_channels,\n\u001b[0;32m    222\u001b[0m     args\u001b[39m.\u001b[39munet_max_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     upsampling_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    236\u001b[0m )\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\human_mesh\\rome\\src\\parametric_avatar.py:154\u001b[0m, in \u001b[0;36mParametricAvatar.__init__\u001b[1;34m(self, image_size, deca_path, use_scalp_deforms, use_neck_deforms, subdivide_mesh, use_details, use_tex, external_params, device)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m# Create distill model\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexternal_params\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39muse_distill\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_linear_model()\n",
      "File \u001b[1;32mc:\\human_mesh\\rome\\src\\parametric_avatar.py:159\u001b[0m, in \u001b[0;36mParametricAvatar._setup_linear_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m n_online_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflame_config\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mn_exp \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflame_config\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mn_pose \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflame_config\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mn_cam\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhair_basis_reg \u001b[39m=\u001b[39m create_regressor(\u001b[39m'\u001b[39m\u001b[39mresnet50\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexternal_params\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mn_scalp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m60\u001b[39m))\n\u001b[1;32m--> 159\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexternal_params[\u001b[39m'\u001b[39;49m\u001b[39mpath_to_linear_hair_model\u001b[39;49m\u001b[39m'\u001b[39;49m], map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhair_basis_reg\u001b[39m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhair_basis_reg\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\tonyl\\miniconda3\\envs\\rome\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\tonyl\\miniconda3\\envs\\rome\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\tonyl\\miniconda3\\envs\\rome\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/linear_hair.pth'"
     ]
    }
   ],
   "source": [
    "from infer import Infer\n",
    "infer = Infer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sources_root \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/imgs/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m src_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m source_lists \u001b[39m=\u001b[39m glob(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msources_root\u001b[39m}\u001b[39;00m\u001b[39m/*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m transform_source \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m infer\u001b[39m.\u001b[39msource_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      9\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "sources_root = 'data/imgs/'\n",
    "\n",
    "src_index = 0\n",
    "source_lists = glob(f'{sources_root}/*')\n",
    "\n",
    "transform_source = True\n",
    "infer.source_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "source_image  = Image.open('data/imgs/lincoln.jpg')\n",
    "driver_img  = Image.open('data/imgs/taras2.jpg')\n",
    "\n",
    "# swap\n",
    "# driver_img, source_image = source_image, driver_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = infer.evaluate(source_image, driver_img, crop_center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "res = tensor2image(torch.cat([out['source_information']['data_dict']['source_img'][0].cpu(),\n",
    "                              out['source_information']['data_dict']['target_img'][0].cpu(),\n",
    "                        out['render_masked'].cpu(), out['pred_target_shape_img'][0].cpu()], dim=2))\n",
    "plt.imshow(res[..., ::-1])\n",
    "plt.axis('off');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = infer.evaluate(source_image, Image.open('data/imgs/taras1.jpg'),\n",
    "                        source_information_for_reuse=out['source_information'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "key_to_show = 'pred_target_shape_img'\n",
    "\n",
    "\n",
    "image_cat = np.concatenate([tensor2image(out[key_to_show][0]),\n",
    "                tensor2image(new_out[key_to_show][0])], axis=1)\n",
    "plt.imshow(image_cat[..., ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video inference\n",
    "\n",
    "Download video with Jenya, thanks him a lot for this example:\n",
    "\n",
    "```sh\n",
    "wget https://www.dropbox.com/s/pht8dd901ff3vzy/jenya_driver.zip -O jenya_driver.zip\n",
    "unzip jenya_driver.zip\n",
    "mkdir data/video\n",
    "mv jenya_driver data/video/\n",
    "rm jenya_driver.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "out = dict()\n",
    "resulted_imgs = defaultdict(list)\n",
    "\n",
    "video_folder = 'data/video/jenya_driver/'\n",
    "image_frames = sorted(glob(f\"{video_folder}/*\", recursive=True), key=lambda x: int(x.split('/')[-1][:-4]))\n",
    "\n",
    "source_image = Image.open('data/imgs/lincoln.jpg')\n",
    "\n",
    "mask_hard_threshold = 0.5\n",
    "N = len(image_frames)//20\n",
    "for i in tqdm(range(0, N, 4)):\n",
    "    new_out = infer.evaluate(source_image, Image.open(image_frames[i]),\n",
    "                        source_information_for_reuse=out.get('source_information'))\n",
    "    \n",
    "    mask_pred = (new_out['pred_target_unet_mask'].cpu() > mask_hard_threshold).float()\n",
    "    mask_pred = mask_errosion(mask_pred[0].float().numpy() * 255)\n",
    "    render = new_out['pred_target_img'].cpu() * (mask_pred) + (1 - mask_pred)\n",
    "        \n",
    "    normals = process_black_shape(((new_out['pred_target_normal'][0].cpu() + 1) / 2 * mask_pred + (1 - mask_pred) ) )\n",
    "    normals[normals==0.5]=1.\n",
    "    \n",
    "    resulted_imgs['res_normal'].append(tensor2image(normals))\n",
    "    resulted_imgs['res_mesh_images'].append(tensor2image(new_out['pred_target_shape_img'][0]))\n",
    "    resulted_imgs['res_renders'].append(tensor2image(render[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resulted_imgs['res_normal'][0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show video results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(resulted_imgs['res_renders'])\n",
    "\n",
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,::-1])\n",
    "plt.axis('off')\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,::-1])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,::-1])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=video.shape[0], interval=30)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference over the folder of source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Infer over folder\n",
    "result_root_dir = 'result_examples'\n",
    "os.makedirs(result_root_dir,exist_ok=True )\n",
    "\n",
    "sources_root = 'examples'\n",
    "source_lists = glob(f'{sources_root}/*')\n",
    "image_frames = sorted(glob(f\"{video_folder}/*\", recursive=True), key=lambda x: int(x.split('/')[-1][:-4]))\n",
    "\n",
    "for src_img_path in source_lists[2:]:\n",
    "    source_name = src_img_path.split('/')[-1][:-4]\n",
    "    source_image = Image.open(src_img_path)\n",
    "#     source_image.thumbnail((256,256), Image.ANTIALIAS)\n",
    "#     data_dict = infer.process_source_for_input_dict(source_image, infer.source_transform)\n",
    "    N = len(image_frames)\n",
    "    resulted_imgs = defaultdict(list)\n",
    "    new_out = dict()\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        new_out = infer.evaluate(source_image, Image.open(image_frames[i]),\n",
    "                            source_information_for_reuse=new_out.get('source_information'), crop_center=True)\n",
    "\n",
    "        mask_pred = (new_out['pred_target_unet_mask'].cpu() > mask_hard_threshold).float()\n",
    "        mask_pred = mask_errosion(mask_pred[0].float().numpy() * 255)\n",
    "        render = new_out['pred_target_img'].cpu() * (mask_pred) + (1 - mask_pred)\n",
    "\n",
    "        normals = process_black_shape(((new_out['pred_target_normal'][0].cpu() + 1) / 2 * mask_pred + (1 - mask_pred) ) )\n",
    "        normals[normals==0.5]=1.\n",
    "\n",
    "        resulted_imgs['res_normal'].append(tensor2image(normals))\n",
    "        resulted_imgs['res_renders'].append(tensor2image(render[0]))\n",
    "    \n",
    "    out = cv2.VideoWriter(f'{result_root_dir}/render_{source_name}.mp4',\n",
    "                          cv2.VideoWriter_fourcc(*'mp4v'), 30, (256, 256))\n",
    "    for i in resulted_imgs['res_renders']:\n",
    "        out.write(i)\n",
    "    out.release()\n",
    "    \n",
    "    out = cv2.VideoWriter(f'{result_root_dir}/normal_{source_name}.mp4', \n",
    "                          cv2.VideoWriter_fourcc(*'mp4v'), 30, (256, 256))\n",
    "    for i in resulted_imgs['res_normal']:\n",
    "        out.write(i)\n",
    "    out.release()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
